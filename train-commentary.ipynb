{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12270133,"sourceType":"datasetVersion","datasetId":7711526}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -r /kaggle/input/final-data/requirements_clip.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport clip\nimport cv2\nimport numpy as np\nfrom PIL import Image \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, get_linear_schedule_with_warmup\nfrom torch.optim import AdamW # Corrected import\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.optim as optim\nimport json\nimport os\nimport time\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T14:02:14.553242Z","iopub.execute_input":"2025-06-24T14:02:14.554369Z","iopub.status.idle":"2025-06-24T14:02:14.558943Z","shell.execute_reply.started":"2025-06-24T14:02:14.554342Z","shell.execute_reply":"2025-06-24T14:02:14.558179Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class CricketCommentaryDataset(Dataset):\n    def __init__(self, annotations, clip_model, preprocess, num_frames=8):\n        self.annotations = annotations\n        self.clip_model = clip_model\n        self.preprocess = preprocess\n        self.num_frames = num_frames\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def extract_frames(self, video_path, start_time, end_time):\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            print(\"video not opened\")\n            return torch.zeros(self.num_frames, 3, 224, 224)\n\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        start_frame = int(start_time * fps)\n        end_frame = int(end_time * fps)\n\n        if start_frame >= end_frame:\n            return torch.zeros(self.num_frames, 3, 224, 224)\n\n        stride = max(1, (end_frame - start_frame) // self.num_frames)\n        frames = []\n\n        for i in range(start_frame, end_frame, stride):\n            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n            ret, frame = cap.read()\n            if ret:\n                # Action-focused cropping\n                h, w, _ = frame.shape\n                crop_size = min(h, w) // 2\n                y_start = max(0, (h - crop_size) // 2)\n                x_start = max(0, (w - crop_size) // 2)\n                cropped = frame[y_start:y_start+crop_size, x_start:x_start+crop_size]\n\n                cropped = cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB)\n                pil_image = Image.fromarray(cropped)\n                frames.append(self.preprocess(pil_image))\n            if len(frames) >= self.num_frames:\n                break\n        \n        # Always ensure we return exactly num_frames\n        if len(frames) < self.num_frames:\n            num_pad = self.num_frames - len(frames)\n            frames.extend([torch.zeros(3, 224, 224)] * num_pad)\n\n        cap.release()\n        return torch.stack(frames)\n\n    def __getitem__(self, idx):\n        ann = self.annotations[idx]\n        frames = self.extract_frames(\n            ann[\"video_path\"],\n            ann[\"start_time\"],\n            ann[\"end_time\"]\n        )\n\n        # Use the prompt and response directly\n        prompt = ann[\"prompt\"]\n        response = ann[\"response\"]\n\n        return {\n            \"frames\": frames,\n            \"prompt\": prompt,\n            \"response\": response\n        }\n    \nclass TemporalTransformerEncoder(nn.Module):\n    def __init__(self, embed_dim, num_heads, num_layers, num_frames, dropout=0.1):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_frames = num_frames\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n\n        self.position_embed = nn.Parameter(torch.zeros(1, num_frames + 1, embed_dim))\n        nn.init.trunc_normal_(self.position_embed, std=0.02)\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=num_heads,\n            dim_feedforward=4 * embed_dim,\n            dropout=dropout,\n            activation='gelu',\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n    def forward(self, x):\n        B = x.size(0)\n        cls_token = self.cls_token.expand(B, 1, -1)\n        x = torch.cat([cls_token, x], dim=1)\n        x = x + self.position_embed[:, :x.size(1)]\n        x = self.transformer(x)\n        return {\n            \"cls\": x[:, 0],\n            \"tokens\": x[:, 1:]\n        }\n\nclass CricketCommentator(nn.Module):\n    def __init__(self, train_mode=False, num_frames=8, gpt2_train_layers=2):\n        super().__init__()\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.num_frames = num_frames\n\n        import clip\n        self.clip, self.preprocess = clip.load(\"ViT-B/32\", device=self.device)\n        self.clip = self.clip.float()\n\n        if train_mode:\n            for param in self.clip.parameters():\n                param.requires_grad = False\n\n        self.temporal_encoder = TemporalTransformerEncoder(\n            embed_dim=512,\n            num_heads=8,\n            num_layers=3,\n            num_frames=num_frames,\n            dropout=0.1\n        ).to(self.device).float()\n\n        self.projection = nn.Sequential(\n            nn.Linear(512, 1024),\n            nn.GELU(),\n            nn.LayerNorm(1024),\n            nn.Dropout(0.1),\n            nn.Linear(1024, 1024),\n            nn.Tanh()\n        ).to(self.device).float()\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\").to(self.device).float()\n        self.gpt2.config.pad_token_id = self.tokenizer.eos_token_id\n\n        # ðŸ”’ Freeze all GPT-2 parameters\n        for param in self.gpt2.parameters():\n            param.requires_grad = False\n\n        # ðŸ”“ Unfreeze last N transformer blocks\n        if train_mode and gpt2_train_layers > 0:\n            for block in self.gpt2.transformer.h[-gpt2_train_layers:]:\n                for param in block.parameters():\n                    param.requires_grad = True\n\n            for param in self.gpt2.lm_head.parameters():\n                param.requires_grad = True\n            for param in self.gpt2.transformer.ln_f.parameters():\n                param.requires_grad = True\n\n    def forward(self, frames):\n        batch_size = frames.shape[0]\n        frames = frames.view(-1, 3, 224, 224)\n        with torch.no_grad():\n            frame_features = self.clip.encode_image(frames.to(self.device))\n        frame_features = frame_features.view(batch_size, self.num_frames, -1).float()\n        frame_features = F.normalize(frame_features, p=2, dim=-1)\n\n        temporal_out = self.temporal_encoder(frame_features)\n        visual_embeds = self.projection(temporal_out[\"cls\"])\n        return F.normalize(visual_embeds, p=2, dim=-1).unsqueeze(1)\n\n    def compute_loss(self, batch):\n        frames = batch[\"frames\"].to(self.device)\n        prompts = batch[\"prompt\"]\n        responses = batch[\"response\"]\n\n        visual_embeds = self.forward(frames)\n\n        full_texts = [f\"{p} {r}\" for p, r in zip(prompts, responses)]\n        inputs = self.tokenizer(\n            full_texts,\n            return_tensors=\"pt\",\n            padding='longest',\n            truncation=True,\n            max_length=128\n        ).to(self.device)\n\n        prompt_inputs = self.tokenizer(\n            prompts,\n            return_tensors=\"pt\",\n            padding='longest',\n            truncation=True,\n            max_length=128\n        ).to(self.device)\n        prompt_lengths = prompt_inputs.attention_mask.sum(dim=1)\n\n        text_embeddings = self.gpt2.transformer.wte(inputs.input_ids)\n        input_embeddings = torch.cat([visual_embeds, text_embeddings], dim=1)\n\n        visual_mask = torch.ones(visual_embeds.shape[:2]).to(self.device)\n        combined_mask = torch.cat([visual_mask, inputs.attention_mask], dim=1)\n\n        labels = inputs.input_ids.clone()\n        labels = torch.cat([\n            -100 * torch.ones(labels.size(0), 1, dtype=torch.long).to(self.device),\n            labels\n        ], dim=1)\n\n        for i, plen in enumerate(prompt_lengths):\n            labels[i, 1:1 + plen] = -100\n\n        outputs = self.gpt2(\n            inputs_embeds=input_embeddings,\n            attention_mask=combined_mask,\n            labels=labels\n        )\n\n        return outputs.loss\n\n\ndef collate_fn(batch):\n    \"\"\"Custom collate function to handle frames\"\"\"\n    frames = [item[\"frames\"] for item in batch]\n    prompts = [item[\"prompt\"] for item in batch]\n    responses = [item[\"response\"] for item in batch]\n    \n    # Stack all frames\n    frames_tensor = torch.stack(frames)\n    \n    return {\n        \"frames\": frames_tensor,\n        \"prompt\": prompts,\n        \"response\": responses\n    }\n\ndef train_model(model, train_loader, val_loader, epochs, lr):\n    \"\"\"Train visual projection layers with enhanced settings\"\"\"\n    device = model.device\n    # Collect GPT-2 trainable parameters\n    gpt2_trainable = [p for p in model.gpt2.parameters() if p.requires_grad]\n \n    # Unfreeze temporal encoder and projection\n    for param in model.temporal_encoder.parameters():\n        param.requires_grad = True\n    for param in model.projection.parameters():\n        param.requires_grad = True\n        \n    optimizer = AdamW([\n        {'params': model.temporal_encoder.parameters(), 'lr': lr},\n        {'params': model.projection.parameters(), 'lr': lr},\n        {'params': gpt2_trainable, 'lr': lr * 0.5}\n    ], weight_decay=0.01)\n    \n    # Learning rate schedulers\n    total_steps = len(train_loader) * epochs\n    warmup_scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(0.1 * total_steps),\n        num_training_steps=total_steps\n    )\n    \n    plateau_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='min',\n        factor=0.5,\n        patience=3,\n        verbose=True\n    )\n    \n    # Early stopping\n    best_val_loss = float('inf')\n    early_stop_patience = 5\n    epochs_no_improve = 0\n    early_stop = False\n    \n    for epoch in range(epochs):\n        if early_stop:\n            print(f\"Early stopping triggered at epoch {epoch+1}\")\n            break\n            \n        print(f\"Epoch {epoch+1}/{epochs}\")\n        model.train()\n        train_loss = 0.0\n        \n        # Training\n        for batch in tqdm(train_loader, desc=\"Training\"):\n            optimizer.zero_grad()\n            loss = model.compute_loss(batch)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            warmup_scheduler.step()\n            train_loss += loss.item()\n        \n        avg_train_loss = train_loss / len(train_loader)\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=\"Validation\"):\n                loss = model.compute_loss(batch)\n                val_loss += loss.item()\n        \n        avg_val_loss = val_loss / len(val_loader)\n        plateau_scheduler.step(avg_val_loss)\n        current_lr = optimizer.param_groups[0]['lr']\n        \n        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {current_lr:.2e}\")\n        \n        # Early stopping check\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            epochs_no_improve = 0\n            torch.save(model.state_dict(), \"best_cricket_commentator.pth\")\n            print(\"Saved best model\")\n        else:\n            epochs_no_improve += 1\n            print(f\"No improvement for {epochs_no_improve}/{early_stop_patience} epochs\")\n            if epochs_no_improve >= early_stop_patience:\n                early_stop = True\n    \n    # Load best model\n    model.load_state_dict(torch.load(\"best_cricket_commentator.pth\"))\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T14:02:16.954807Z","iopub.execute_input":"2025-06-24T14:02:16.955230Z","iopub.status.idle":"2025-06-24T14:02:16.985398Z","shell.execute_reply.started":"2025-06-24T14:02:16.955202Z","shell.execute_reply":"2025-06-24T14:02:16.984670Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load annotations\nwith open(\"/kaggle/input/final-data/Json_360B.json\", \"r\") as f:\n    annotations = json.load(f)\n\n# Split into train and validation (85/15)\nsplit_idx = int(0.85 * len(annotations))\ntrain_annotations = annotations[:split_idx]\nval_annotations = annotations[split_idx:]\n\n# Initialize CLIP for dataset\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nclip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n\n# Create datasets\ntrain_dataset = CricketCommentaryDataset(\n    train_annotations,\n    clip_model,\n    preprocess,\n    num_frames=8\n)\nval_dataset = CricketCommentaryDataset(\n    val_annotations,\n    clip_model,\n    preprocess,\n    num_frames=8\n)\n\n# Create data loaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=8,  # Small batch size due to memory constraints\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=2\n)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=4,\n    collate_fn=collate_fn,\n    num_workers=2\n)\n\n# Initialize model in training mode\nmodel = CricketCommentator(train_mode=True).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T14:02:48.361398Z","iopub.execute_input":"2025-06-24T14:02:48.362157Z","iopub.status.idle":"2025-06-24T14:06:01.682038Z","shell.execute_reply.started":"2025-06-24T14:02:48.362123Z","shell.execute_reply":"2025-06-24T14:06:01.681416Z"}},"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338M/338M [02:48<00:00, 2.10MiB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23c37e70eddc49c7bc747b17945d6cfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3cfa620a1c344a781b847897f2f45a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4348f5900bb84b42b73c6385b4429f59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d0ea606fef64d42a1d7d917d39d6805"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed1d6fd9819d4c4ab184db8d97c76018"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b7748ed512247729accab6f0a955bcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffb92f97702c412ea2a341d4a6e30f6d"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Train the model\ntrained_model = train_model(model,train_loader,val_loader,epochs=30,lr=1e-4)\n  # Save final model\ntorch.save(trained_model.state_dict(), \"cricket_commentator_final.pth\")\nprint(\"Model saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T14:06:38.706726Z","iopub.execute_input":"2025-06-24T14:06:38.707568Z","iopub.status.idle":"2025-06-24T15:50:06.798898Z","shell.execute_reply.started":"2025-06-24T14:06:38.707531Z","shell.execute_reply":"2025-06-24T15:50:06.797920Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/39 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\nTraining: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [06:38<00:00, 10.22s/it]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:14<00:00,  5.33s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 6.2700 | Val Loss: 4.9263 | LR: 3.33e-05\nSaved best model\nEpoch 2/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [06:36<00:00, 10.16s/it]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:13<00:00,  5.24s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: 4.3325 | Val Loss: 3.4906 | LR: 6.67e-05\nSaved best model\nEpoch 3/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [06:34<00:00, 10.11s/it]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:13<00:00,  5.26s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: 2.8148 | Val Loss: 3.1422 | LR: 1.00e-04\nSaved best model\nEpoch 4/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [06:31<00:00, 10.05s/it]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:13<00:00,  5.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Train Loss: 2.4087 | Val Loss: 3.0739 | LR: 9.63e-05\nSaved best model\nEpoch 5/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [06:38<00:00, 10.21s/it]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:15<00:00,  5.41s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Train Loss: 2.1992 | Val Loss: 3.0188 | LR: 9.26e-05\nSaved best model\nEpoch 6/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [06:40<00:00, 10.26s/it]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:15<00:00,  5.36s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Train Loss: 2.1436 | Val Loss: 3.0054 | LR: 8.89e-05\nSaved best model\nEpoch 7/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [06:41<00:00, 10.30s/it]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:15<00:00,  5.43s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Train Loss: 2.0236 | Val Loss: 2.9600 | LR: 8.52e-05\nSaved best model\nEpoch 8/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [06:42<00:00, 10.31s/it]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:14<00:00,  5.33s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 | Train Loss: 1.9391 | Val Loss: 2.9476 | LR: 8.15e-05\nSaved best model\nEpoch 9/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [06:41<00:00, 10.30s/it]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:15<00:00,  5.38s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 | Train Loss: 1.8908 | Val Loss: 2.9669 | LR: 7.78e-05\nNo improvement for 1/5 epochs\nEpoch 10/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [06:44<00:00, 10.37s/it]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:15<00:00,  5.39s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 1.8204 | Val Loss: 2.9698 | LR: 7.41e-05\nNo improvement for 2/5 epochs\nEpoch 11/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [06:45<00:00, 10.39s/it]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:14<00:00,  5.35s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Train Loss: 1.8060 | Val Loss: 2.9781 | LR: 7.04e-05\nNo improvement for 3/5 epochs\nEpoch 12/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [06:43<00:00, 10.35s/it]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:14<00:00,  5.34s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Train Loss: 1.7672 | Val Loss: 2.9871 | LR: 3.33e-05\nNo improvement for 4/5 epochs\nEpoch 13/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [06:36<00:00, 10.16s/it]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:13<00:00,  5.23s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Train Loss: 1.6623 | Val Loss: 2.9942 | LR: 6.30e-05\nNo improvement for 5/5 epochs\nEarly stopping triggered at epoch 14\nModel saved successfully!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"trained_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T15:53:18.239157Z","iopub.execute_input":"2025-06-24T15:53:18.239507Z","iopub.status.idle":"2025-06-24T15:53:18.251521Z","shell.execute_reply.started":"2025-06-24T15:53:18.239478Z","shell.execute_reply":"2025-06-24T15:53:18.250720Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"CricketCommentator(\n  (clip): CLIP(\n    (visual): VisionTransformer(\n      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (transformer): Transformer(\n        (resblocks): Sequential(\n          (0): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (1): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (2): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (3): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (4): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (5): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (6): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (7): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (8): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (9): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (10): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (11): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (transformer): Transformer(\n      (resblocks): Sequential(\n        (0): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (6): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (7): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (8): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (9): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (10): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (11): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (token_embedding): Embedding(49408, 512)\n    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n  (temporal_encoder): TemporalTransformerEncoder(\n    (transformer): TransformerEncoder(\n      (layers): ModuleList(\n        (0-2): 3 x TransformerEncoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (projection): Sequential(\n    (0): Linear(in_features=512, out_features=1024, bias=True)\n    (1): GELU(approximate='none')\n    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    (3): Dropout(p=0.1, inplace=False)\n    (4): Linear(in_features=1024, out_features=1024, bias=True)\n    (5): Tanh()\n  )\n  (gpt2): GPT2LMHeadModel(\n    (transformer): GPT2Model(\n      (wte): Embedding(50257, 1024)\n      (wpe): Embedding(1024, 1024)\n      (drop): Dropout(p=0.1, inplace=False)\n      (h): ModuleList(\n        (0-23): 24 x GPT2Block(\n          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D(nf=3072, nx=1024)\n            (c_proj): Conv1D(nf=1024, nx=1024)\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D(nf=4096, nx=1024)\n            (c_proj): Conv1D(nf=1024, nx=4096)\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n    (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n  )\n)"},"metadata":{}}],"execution_count":6}]}